<html>
<head>
<title>Listen to Look into the Future: Audio-Visual Egocentric Gaze Anticipation</title>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@200&display=swap" rel="stylesheet"> 
<style type="text/css">
.content {
    width:930px;
    text-align: left;
    font-family: 'Open Sans', sans-serif;
    font-weight: 200;
}
h1 {
    font-weight: 600;
}
table.authors {
    width:90%;
    text-align:center;
}
table.authors > tr > td, table.authors > tbody > tr > td {
    width:25%;
}
.authors a, .lnk {
    color:rgb(82, 147, 221);
    font-size:120%;
    /* text-decoration:underline; */
}
.authors a:hover, .lnk:hover {
    color:gray;
    font-size:120%;
    /* text-decoration:underline; */
}
.btn {
    color:black;
    text-decoration:none;
}
.btn:hover {
    color:gray;
    text-decoration:none;
}
table.demo1 {
    width:100%;
    text-align:center;
}
.demo1 img {
    width:300px;
}
td.prompt {
    width:100%;
    text-align:center;
    font-family: monospace;
    font-size:150%;
}
td.prompt a {
    color:#ddd;
    text-decoration:none;
}
td.prompt a:hover, td.prompt a.active {
    color:black;
    text-decoration:none;
}
.img-stack {
    position:relative;
    display: block;
    width:300px;
    height:300px;
}
.img-stack img {
    position: absolute;
    top: 0px;
    left: 0px;
    z-index: 0;
}
.img-stack img.active {
    z-index: 1;
}
.img-stack .overlay {
    width: 300px;
    height: 300px;
    opacity: 0;
    transition: opacity .2s;
    z-index: 2;
    position: absolute;
    top: 0px;
    left: 0px;
    background: white;
}
.carousel {
    position:relative;
    width:650px;
    height:340px;
    overflow:hidden;
}
.carousel > table {
    position:absolute;
    top: 0px;
    transition: left 1s;
    width:650px;
}
.carousel_table td {
    text-align:center;
    font-size: 130%;
}
.carousel_table td:nth-child(2) {
    font-size:150%;
}
pre {
    background-color:#eee;
    border: 1px solid #999;
    border-radius: 5px;
    padding: 10px;
white-space: break-spaces;
width:80%;
text-align:left;
}
td.gif {
    width: 33%;
    font-family: monospace;
    font-size: 120%;
}
.dl_link {
    display: inline-block;
    padding-right: 6px;
    padding-left: 6px;
    padding-top: 2px;
    padding-bottom: 2px;
}
.dl_link, .dl_link td {
    color: black;
    text-decoration: none;
    font-size: 120%;
}
.dl_link, .dl_link table {
    border-radius: 5px;
    background-color: none;
}
.dl_link:hover, .dl_link:hover * {
    color: #404040 !important;
    background-color: #d9d9d9 !important;
}
@media only screen and (max-width: 930px) {
    .content { width:100% !important; }
}
</style>
<script type="text/javascript">
  var curr_idx = 0;
  var num_tables = 5;
  function move(direction) {
      if (curr_idx == 0 && direction < 0) {
	  move(num_tables - 1);
	  return;
      }
      if (curr_idx == num_tables - 1 && direction > 0) {
	  move(-1 * (num_tables - 1));
	  return;
      }
      tables = document.getElementsByClassName("carousel_table");
      for(var i = 0; i < tables.length; i++) {
	  tables[i].style.left = (parseInt(tables[i].style.left.substring(0, tables[i].style.left.length - 2)) - direction * 650).toString() + "px";
      }
      curr_idx += direction;
  }
  function hideOverlay(classname) {
      document.getElementById(classname + "_overlay").style.opacity = "0";
  }
  function activate(classname, idx, max) {
      document.getElementById(classname + "_overlay").style.opacity = "1";
      setTimeout(function() {
	  for(var i = 1; i <= max; i++) {
	      document.getElementById(classname + "_" + i.toString()).className = "";
	      document.getElementById(classname + "_text_" + i.toString()).className = "";
	  }
	  document.getElementById(classname + "_" + idx.toString()).className = "active";
	  document.getElementById(classname + "_text_" + idx.toString()).className = "active";
	  setTimeout(hideOverlay, 200, classname);
      }, 200);
  }
  var moving = true;
  var stopCounter = 0;
  function autoMove(currCounter) {
      if(moving && currCounter >= stopCounter) {
	  move(1);
          setTimeout(autoMove, 5000, stopCounter);
      }
  }
  function beginMoving() {
      moving = true;
      setTimeout(autoMove, 5000, stopCounter);
  }
  function stopMoving() {
      moving = false;
      stopCounter++;
  }
</script>
</head>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
<body dir="ltr" onload="beginMoving();">
<center><div class="content">
    <!----------------------------------- Title and Authors ----------------------------------->
    <center>
      <br>
      <br>
      <h1>Listen to Look into the Future: Audio-Visual Egocentric Gaze Anticipation</h1>
      <table class="authors">
        <td><span style="font-size: 23px; color:darkred">ECCV 2024</span></td>
      </table>
      <br>
      <table class="authors">
        <tr>
          <td><a href="https://bolinlai.github.io/">Bolin Lai</a><sup>1</sup></td>
          <td><a href="https://fkryan.github.io/">Fiona Ryan</a><sup>1</sup></td>
          <td><a href="https://vjwq.github.io/">Wenqi Jia</a><sup>1</sup></td>
        </tr>
        <tr>
          <td><a href="https://aptx4869lm.github.io/">Miao Liu</a><sup>2,</sup>*</td>
          <td><a href="https://rehg.org/">James M. Rehg</a><sup>3,</sup>*</td>
        </tr>
      </table>
      <!-- <br> -->
      <div align="right">* Equal corresponding author</div>
      <br>
      <table class="authors">
        <td><span style="font-size: 19px"><sup>1</sup>Georgia Institute of Technology</span></td>
        <td><span style="font-size: 19px"><sup>2</sup>GenAI, Meta</span></td>
        <td><span style="font-size: 19px"><sup>3</sup>University of Illinois Urbana-Champaign</span></td>
      </table>
      <br>
      <table class="authors">
        <td>
          <a href="https://arxiv.org/pdf/2305.03907.pdf"><b>[Arixv]</b></a> &nbsp;&nbsp;&nbsp;&nbsp;
          <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01396.pdf"><b>[Camera Ready]</b></a> &nbsp;&nbsp;&nbsp;&nbsp;
          <a href="https://github.com/BolinLai/CSTS"><b>[Code]</b></a> &nbsp;&nbsp;&nbsp;&nbsp;
          <a href="https://github.com/BolinLai/CSTS/tree/main/data"><b>[Dataset Split]</b></a> &nbsp;&nbsp;&nbsp;&nbsp;
          <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01396-supp.pdf"><b>[Supplementary]</b></a> &nbsp;&nbsp;&nbsp;&nbsp;
          <a href="figures/poster.pdf"><b>[Poster]</b></a>
        </td>
      </table>
      <br>
      <br>
    </center>

    <!----------------------------------- Teaser Image ----------------------------------->
    <div style="text-align: center;">
      <img src="figures/teaser.png"  style="width:100%;">
    </div>
    <br>
    <p>
      In the input sequence, the camera view shifts from the paper held by the camera wearer to the standing speaker who asks a question. Then the sitting speaker on the far right answers the question, which is captured by the audio stream. In the anticipation sequence, the camera wearer's gaze shifts towards the sitting person's head after hearing her response. In this case, the audio stream (the sitting person's response) is an important stimulus that triggers this gaze movement.
    </p>
    <br>

    <div id="examples_ego4d">
      <center>
        <video controls="controls" width="300">
          <source src="figures/demo1.mp4" type="video/mp4">
        </video>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        <video controls="controls" width="300">
          <source src="figures/demo2.mp4" type="video/mp4">
        </video>
      </center>
    </div>
    <br>
    
    <!----------------------------------- Abstract ----------------------------------->
    <div id="abstract" style="border-top:1px solid gray;">
      <h2>Abstract</h2>
      <p>
        Egocentric gaze anticipation serves as a key building block for the emerging capability of Augmented Reality. Notably, gaze behavior is driven by both visual cues and audio signals during daily activities. Motivated by this observation, we introduce the first model that leverages both the video and audio modalities for egocentric gaze anticipation. Specifically, we propose a Contrastive Spatial-Temporal Separable (CSTS) fusion approach that adopts two modules to separately capture audio-visual correlations in spatial and temporal dimensions, and applies a contrastive loss on the re-weighted audio-visual features from fusion modules for representation learning. We conduct extensive ablation studies and thorough analysis using two egocentric video datasets: Ego4D and Aria, to validate our model design. We demonstrate the audio improves the performance by +2.5% and +2.4% on the two datasets. Our model also outperforms the prior state-of-the-art methods by at least +1.9% and +1.6%. Moreover, we provide visualizations to show the gaze anticipation results and provide additional insights into audio-visual representation learning.
      </p>
    </div>
    <br>

    <!----------------------------------- Method ----------------------------------->
    <div id="method" style="border-top:1px solid gray;">
      <h2>The Proposed Method</h2>
      <center>
        <img src="figures/method.png" style="width:100%;">
      </center>
      <br>
      <p>
        We exploit the transformer-based encoders `phi(x)` and `psi(a)` to extract the representations of the video frames `x` and audio signals `a`. We then employ a <b>C</b>ontrastive <b>S</b>patial-<b>T</b>emporal <b>S</b>eparable  (CSTS) audio-visual fusion approach. Specifically, a spatial fusion module captures the correlation between audio embeddings and spatial appearance-based features; a temporal fusion module captures the temporal correlation between the visual and audio embeddings; and a contrastive loss is applied on fused audio-visual embeddings to facilitate the representation learning. Finally, spatially and temporally fused audio-visual features are merged and fed into a decoder for future gaze anticipation.
      </p>
    </div>
    <br>

    <!----------------------------------- Demo Video ----------------------------------->
    <div id="examples_ego4d" style="border-top:1px solid gray;">
      <h2>Demo Video</h2>
      <center>
        <!-- <video controls="controls" width="720">
          <source src="figures/presentation.mp4" type="video/mp4">
        </video> -->
        <iframe width="720" height="405" src="https://www.youtube.com/embed/5ApMaLhlxUc?si=m9_SOxFTRjLvVIun" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
      </center>
    </div>
    <br>

    <!----------------------------------- Experiments ----------------------------------->
    <div id="curve" style="border-top:1px solid gray;">
      <h2>Experiment Results</h2>
      <center>
        <img src="figures/exp.png" style="width:100%;">
        <br>
        <br>
        <br>
        <img src="figures/curve.png" style="width:100%;">
      </center>
    </div>
    <br>
    <br>

    <!----------------------------------- Visualization ----------------------------------->
    <div id="examples" style="border-top:1px solid gray;">
      <h2>Examples</h2>
      <center>
        <img src="figures/visualization_1.png" style="width:90%;">
      </center>
    </div>
    <br>
    <div id="additional_examples" style="border-top:1px solid gray;">
      <h2>Additional Examples</h2>
      <center>
        <img src="figures/visualization_2.png" style="width:90%;">
      </center>
    </div>
    <br>

    <!----------------------------------- Bibtex ----------------------------------->
    <div id="bibtex" style="border-top:1px solid gray;">
      <h2>BibTeX</h2>
      <center>
      <pre><code>@article{lai2023listen,
        title={Listen to look into the future: Audio-visual egocentric gaze anticipation},
        author={Lai, Bolin and Ryan, Fiona and Jia, Wenqi and Liu, Miao and Rehg, James M},
        journal={arXiv preprint arXiv:2305.03907},
        year={2023}
      }
      </code></pre>
      </center>
    </div>

</div></center>

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</body>
</html>
